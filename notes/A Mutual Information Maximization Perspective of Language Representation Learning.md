---
tags: [self-supervised]
title: A Mutual Information Maximization Perspective of Language Representation Learning
created: '2020-09-23T18:52:49.260Z'
modified: '2020-09-23T18:55:16.749Z'
---

# A Mutual Information Maximization Perspective of Language Representation Learning

## overview

- using [InfoNCE](@note/Representation Learning with Contrastive Predictive Coding) formalism show that skip-gram, BERT, and XLNet are all maximizing MI 

## citation

```
@inproceedings{kong2020-representation83,
    author = {Lingpeng Kong, Cyprien de Masson d'Autume, Lei Yu, Wang Ling, Zihang Dai and Dani Yogatama},
    title = {A Mutual Information Maximization Perspective of Language Representation Learning},
    year = {2020},
    booktitle = {ICLR 2020 : Eighth International Conference on Learning Representations}
}
```
